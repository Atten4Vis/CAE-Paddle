# 模型说明

基于海量的互联网商品图文信息，百度提出多源信息统一建模的商品图文表征预训练模型 VIMER-UMS (Unified Multi-Source Pre-training for Product)，是行业首个统一视觉单模态与多源图文模态表征的商品多模态预训练模型。针对图文多模态建模中模态信息残缺问题，通过构建视觉特征与多源图文对比的多任务学习框架，实现统一图文表征预训练同时覆盖商品视觉单模态、多模态识别与检索任务，显著优化商品视觉检索、广告识别、多模态商品搜索推荐体验，高效提升线下零售商品识别效果，解决小样本定制优化痛点。


# 原理介绍
现有多模态图文预训练方法主要面向图文跨模态搜索、多模态理解与生成任务，侧重对图文模态特征的关系表征，对单模态视觉下游任务效果支持不足。以 OpenAI CLIP、Google Align 为代表的大规模图文预训练方法依赖大量训练资源及亿级大数据，高昂成本制约多模态大模型的规模化应用。
此外，真实场景中的多模态关联数据不仅限于简单的图文对。相比两维的图文对形式，多源信息是指具有多维度的信息来源，以商品搜索场景为例，包括文本模态（搜索输入、场景文字、文本标题、类目标签）、视觉模态（商品图、同款标签）的多维多模态信息，其中蕴含丰富的语义关联，具有极大的挖掘利用潜力与应用价值。然而，在实际应用中，多源商品信息通常存在模态信息缺失的问题，是多源信息模态建模应用面临的重要挑战。
